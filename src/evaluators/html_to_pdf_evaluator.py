"""
HTMLè½¬PDFè¯„ä¼°å™¨
è´Ÿè´£è¯„ä¼°è½¬æ¢ç»“æœå¹¶è®¡ç®—è¯„åˆ†
"""

import os
from typing import Dict, List, Any
from models.evaluation_models import (
    SampleResult, EvaluationMetrics, EVALUATION_DIMENSIONS, 
    SAMPLES_INFO, SAMPLE_WEIGHTS,
    calculate_dynamic_quality_score, calculate_dimension_scores
)
from models.objective_evaluation import ObjectiveEvaluator, ObjectiveMetrics
from utils.test_runner import TestRunner
from utils.file_operations import FileOperations
from utils.pdf_analyzer import PDFAnalyzer
from generators.html_report_generator import HTMLReportGenerator


class HTMLToPDFEvaluator:
    """HTMLè½¬PDFè¯„ä¼°å™¨"""
    
    def __init__(self, output_dir: str = "output"):
        self.output_dir = output_dir
        self.samples_info = SAMPLES_INFO
        self.sample_weights = SAMPLE_WEIGHTS
        self.evaluation_dimensions = EVALUATION_DIMENSIONS
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.test_runner = TestRunner()
        self.file_ops = FileOperations()
        self.html_generator = HTMLReportGenerator()
        self.pdf_analyzer = PDFAnalyzer()
        self.objective_evaluator = ObjectiveEvaluator()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.file_ops.ensure_directory_exists(output_dir)
    
    def convert_raw_results_to_sample_results(self, raw_results: Dict[str, List[Dict]]) -> Dict[str, List[SampleResult]]:
        """å°†åŸå§‹æµ‹è¯•ç»“æœè½¬æ¢ä¸ºSampleResultå¯¹è±¡"""
        converted_results = {}
        
        for tool_name, results in raw_results.items():
            sample_results = []
            for result in results:
                sample_result = SampleResult(
                    sample_name=result.get('sample', 'unknown'),
                    file_path=result.get('file_path', ''),
                    conversion_success=result.get('success', False),
                    conversion_time=result.get('time', 0.0),
                    file_size=result.get('file_size', 0),
                    error_message=result.get('error', ''),
                    quality_score=result.get('quality_score', 0.0)
                )
                sample_results.append(sample_result)
            converted_results[tool_name] = sample_results
        
        return converted_results
    
    def calculate_quality_score(self, sample_name: str, success: bool, 
                              conversion_time: float, file_size: int) -> float:
        """è®¡ç®—è´¨é‡è¯„åˆ†"""
        if not success:
            return 0.0
        
        # åŸºç¡€åˆ†æ•° (50åˆ†)
        base_score = 50
        
        # æ—¶é—´è¯„åˆ† (30åˆ†)
        time_score = 0
        if conversion_time <= 0.5:
            time_score = 30
        elif conversion_time <= 1.0:
            time_score = 25
        elif conversion_time <= 2.0:
            time_score = 20
        elif conversion_time <= 5.0:
            time_score = 15
        else:
            time_score = 10
        
        # æ–‡ä»¶å¤§å°è¯„åˆ† (20åˆ†) - æ ¹æ®æ–‡ä»¶å¤§å°åˆç†æ€§è¯„åˆ†
        size_score = 0
        if file_size <= 50000:  # 50KBä»¥ä¸‹
            size_score = 20
        elif file_size <= 200000:  # 200KBä»¥ä¸‹
            size_score = 15
        elif file_size <= 500000:  # 500KBä»¥ä¸‹
            size_score = 10
        else:
            size_score = 5
        
        total_score = base_score + time_score + size_score
        return min(100.0, total_score)
    
    def calculate_tool_score(self, results: List[SampleResult]) -> float:
        """è®¡ç®—å·¥å…·æ€»åˆ†"""
        if not results:
            return 0.0
        
        total_weighted_score = 0.0
        total_weight = 0.0
        
        for result in results:
            # è·å–æ ·ä¾‹æƒé‡
            weight = SAMPLE_WEIGHTS.get(result.sample_name, 1.0)
            
            quality_score = self.calculate_quality_score(
                result.sample_name,
                result.conversion_success,
                result.conversion_time,
                result.file_size
            )
            
            total_weighted_score += quality_score * weight
            total_weight += weight
        
        return total_weighted_score / total_weight if total_weight > 0 else 0.0
    
    def run_objective_evaluation(self, tool_name: str, pdf_files: List[str]) -> ObjectiveMetrics:
        """è¿è¡Œå®¢è§‚è¯„ä¼°"""
        # åˆ†æPDFæ–‡ä»¶
        pdf_results = {}
        for pdf_file in pdf_files:
            try:
                filename = os.path.basename(pdf_file)
                analysis = self.pdf_analyzer.analyze_pdf(pdf_file)
                pdf_results[filename] = analysis
            except Exception as e:
                print(f"âš ï¸ åˆ†æPDFæ–‡ä»¶å¤±è´¥ {pdf_file}: {e}")
                continue
        
        if not pdf_results:
            # è¿”å›ç©ºçš„å®¢è§‚è¯„ä¼°ç»“æœ
            return ObjectiveMetrics(
                tool_name=tool_name,
                avg_file_size=0.0,
                file_size_consistency=0.0,
                compression_efficiency=0.0,
                text_preservation_rate=0.0,
                content_density_score=0.0,
                chinese_support_score=0.0,
                special_char_support=0.0,
                image_support_rate=0.0,
                font_preservation_rate=0.0,
                form_support_rate=0.0,
                page_structure_score=0.0,
                success_rate=0.0,
                error_rate=0.0,
                overall_score=0.0
            )
        
        # å®šä¹‰åŸå§‹æ ·ä¾‹æ–‡æœ¬ï¼ˆç”¨äºæ–‡æœ¬ä¿ç•™ç‡è®¡ç®—ï¼‰
        original_samples = {
            'base.html': 'åŸºç¡€HTMLé¡µé¢æµ‹è¯•å†…å®¹ï¼ŒåŒ…å«æ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ç­‰åŸºæœ¬å…ƒç´ ã€‚',
            'chinese.html': 'ä¸­æ–‡å†…å®¹æµ‹è¯•é¡µé¢ï¼ŒåŒ…å«å„ç§ä¸­æ–‡å­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šæ ¼å¼ã€‚æµ‹è¯•ä¸­æ–‡å­—ä½“æ¸²æŸ“æ•ˆæœã€‚',
            'complex.html': 'å¤æ‚å¸ƒå±€æµ‹è¯•é¡µé¢ï¼ŒåŒ…å«å¤šåˆ—å¸ƒå±€ã€æµ®åŠ¨å…ƒç´ ã€å®šä½å…ƒç´ ç­‰å¤æ‚CSSæ ·å¼ã€‚',
            'dynamic.html': 'åŠ¨æ€å†…å®¹æµ‹è¯•é¡µé¢ï¼ŒåŒ…å«JavaScriptç”Ÿæˆçš„å†…å®¹å’ŒåŠ¨æ€æ ·å¼ã€‚',
            'forms.html': 'è¡¨å•å…ƒç´ æµ‹è¯•é¡µé¢ï¼ŒåŒ…å«å„ç§è¾“å…¥æ¡†ã€æŒ‰é’®ã€é€‰æ‹©å™¨ç­‰è¡¨å•æ§ä»¶ã€‚',
            'long_document.html': 'é•¿æ–‡æ¡£æµ‹è¯•é¡µé¢ï¼ŒåŒ…å«å¤§é‡æ–‡æœ¬å†…å®¹ã€å¤šä¸ªç« èŠ‚ã€ç›®å½•ç­‰é•¿æ–‡æ¡£ç‰¹æ€§ã€‚',
            'print_styles.html': 'æ‰“å°æ ·å¼æµ‹è¯•é¡µé¢ï¼ŒåŒ…å«ä¸“é—¨çš„æ‰“å°CSSæ ·å¼å’Œåª’ä½“æŸ¥è¯¢ã€‚',
            'special_chars.html': 'ç‰¹æ®Šå­—ç¬¦æµ‹è¯•é¡µé¢ï¼ŒåŒ…å«å„ç§ç‰¹æ®Šç¬¦å·ã€æ•°å­¦å…¬å¼ã€emojiç­‰ç‰¹æ®Šå­—ç¬¦ã€‚',
            'svg.html': 'SVGå›¾å½¢æµ‹è¯•é¡µé¢ï¼ŒåŒ…å«å„ç§SVGå›¾å½¢ã€å›¾æ ‡å’ŒçŸ¢é‡å›¾å½¢å…ƒç´ ã€‚'
        }
        
        # è¿è¡Œå®¢è§‚è¯„ä¼°
        return self.objective_evaluator.evaluate_tool_objectively(tool_name, pdf_results, original_samples)

    def calculate_metrics(self, tool_name: str, results: List[SampleResult]) -> EvaluationMetrics:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
        successful_results = [r for r in results if r.conversion_success]
        conversion_times = [r.conversion_time for r in successful_results if r.conversion_time > 0]
        file_sizes = {r.sample_name: r.file_size for r in results if r.file_size > 0}
        
        # ä½¿ç”¨æ–°çš„åŠ¨æ€è¯„åˆ†ç³»ç»Ÿè®¡ç®—å„ç»´åº¦å¾—åˆ†
        dimension_scores = calculate_dimension_scores(tool_name, results)
        
        return EvaluationMetrics(
            tool_name=tool_name,
            layout_visual_score=dimension_scores["layout_visual"],
            functionality_score=dimension_scores["functionality"],
            performance_score=dimension_scores["performance"],
            deployment_score=dimension_scores["deployment"],
            customization_score=dimension_scores["customization"],
            conversion_times=conversion_times,
            file_sizes=file_sizes,
            sample_results=results
        )
    
    def run_evaluation(self) -> tuple[Dict[str, List[SampleResult]], Dict[str, EvaluationMetrics], Dict[str, ObjectiveMetrics]]:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸš€ å¼€å§‹HTMLè½¬PDFå·¥å…·è¯„ä¼°...")
        
        # è¿è¡Œå®é™…æµ‹è¯•
        raw_results = self.test_runner.run_actual_tests()
        
        # è½¬æ¢ç»“æœæ ¼å¼
        results = self.convert_raw_results_to_sample_results(raw_results)
        
        # é‡æ–°è®¡ç®—è´¨é‡è¯„åˆ†
        for tool_name, tool_results in results.items():
            for result in tool_results:
                if result.conversion_success:
                    result.quality_score = self.calculate_quality_score(
                        result.sample_name,
                        result.conversion_success,
                        result.conversion_time,
                        result.file_size
                    )
        
        # è®¡ç®—ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡
        metrics = {}
        for tool_name, tool_results in results.items():
            metrics[tool_name] = self.calculate_metrics(tool_name, tool_results)
        
        # è¿è¡Œå®¢è§‚è¯„ä¼°
        print("ğŸ“Š å¼€å§‹å®¢è§‚è¯„ä¼°...")
        objective_metrics = {}
        for tool_name, tool_results in results.items():
            # æ”¶é›†è¯¥å·¥å…·çš„PDFæ–‡ä»¶è·¯å¾„
            pdf_files = [result.file_path for result in tool_results if result.conversion_success and result.file_path]
            if pdf_files:
                print(f"ğŸ” è¯„ä¼° {tool_name} å·¥å…· ({len(pdf_files)} ä¸ªPDFæ–‡ä»¶)...")
                objective_metrics[tool_name] = self.run_objective_evaluation(tool_name, pdf_files)
            else:
                print(f"âš ï¸ {tool_name} å·¥å…·æ²¡æœ‰æˆåŠŸçš„PDFæ–‡ä»¶å¯ä¾›è¯„ä¼°")
                objective_metrics[tool_name] = ObjectiveMetrics(
                    tool_name=tool_name,
                    avg_file_size=0.0,
                    file_size_consistency=0.0,
                    compression_efficiency=0.0,
                    text_preservation_rate=0.0,
                    content_density_score=0.0,
                    chinese_support_score=0.0,
                    special_char_support=0.0,
                    image_support_rate=0.0,
                    font_preservation_rate=0.0,
                    form_support_rate=0.0,
                    page_structure_score=0.0,
                    success_rate=0.0,
                    error_rate=0.0,
                    overall_score=0.0
                )
        
        return results, metrics, objective_metrics
    
    def generate_performance_analysis(self, metrics: Dict[str, EvaluationMetrics]) -> str:
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        analysis = []
        analysis.append("ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Š")
        analysis.append("=" * 50)
        
        # æŒ‰æ€»åˆ†æ’åº
        sorted_tools = sorted(metrics.items(), key=lambda x: x[1].calculate_weighted_score(), reverse=True)
        
        analysis.append(f"\nğŸ† ç»¼åˆæ’å:")
        for i, (tool_name, metric) in enumerate(sorted_tools, 1):
            analysis.append(f"{i}. {tool_name}: {metric.calculate_weighted_score():.1f}åˆ†")
        
        # å„ç»´åº¦åˆ†æ
        analysis.append(f"\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
        for tool_name, metric in sorted_tools:
            # è®¡ç®—æˆåŠŸç‡
            successful_count = sum(1 for result in metric.sample_results if result.conversion_success)
            success_rate = (successful_count / len(metric.sample_results) * 100) if metric.sample_results else 0
            
            # è®¡ç®—å¹³å‡å€¼
            avg_time = sum(metric.conversion_times) / len(metric.conversion_times) if metric.conversion_times else 0
            avg_size = sum(metric.file_sizes.values()) / len(metric.file_sizes) if metric.file_sizes else 0
            avg_quality = sum(r.quality_score for r in metric.sample_results) / len(metric.sample_results) if metric.sample_results else 0
            
            analysis.append(f"\n{tool_name}:")
            analysis.append(f"  â€¢ æˆåŠŸç‡: {success_rate:.1f}%")
            analysis.append(f"  â€¢ å¹³å‡è½¬æ¢æ—¶é—´: {avg_time:.2f}s")
            analysis.append(f"  â€¢ å¹³å‡æ–‡ä»¶å¤§å°: {avg_size/1024:.1f}KB")
            analysis.append(f"  â€¢ å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.1f}")
        
        # æ€§èƒ½å¯¹æ¯”
        if len(sorted_tools) >= 2:
            best = sorted_tools[0]
            second = sorted_tools[1]
            analysis.append(f"\nğŸ” æ€§èƒ½å¯¹æ¯”:")
            analysis.append(f"  â€¢ {best[0]} æ¯” {second[0]} é«˜å‡º {best[1].calculate_weighted_score() - second[1].calculate_weighted_score():.1f}åˆ†")
        
        return "\n".join(analysis)
    
    def print_evaluation_report(self, results: Dict[str, List[SampleResult]], 
                              metrics: Dict[str, EvaluationMetrics]) -> None:
        """æ‰“å°è¯„ä¼°æŠ¥å‘Šåˆ°æ§åˆ¶å°"""
        print("\n" + "="*80)
        print("ğŸ“„ HTMLè½¬PDFå·¥å…·è¯„ä¼°æŠ¥å‘Š")
        print("="*80)
        
        # è¯„ä¼°ç»´åº¦è¯´æ˜
        print(f"\nğŸ“‹ è¯„ä¼°ç»´åº¦:")
        for dimension, weight, description in self.evaluation_dimensions:
            print(f"  â€¢ {dimension}: {weight} - {description}")
        
        # å·¥å…·è¯„åˆ†
        print(f"\nğŸ† å·¥å…·è¯„åˆ† (åŠ æƒæ€»åˆ†):")
        sorted_tools = sorted(metrics.items(), key=lambda x: x[1].calculate_weighted_score(), reverse=True)
        for i, (tool_name, metric) in enumerate(sorted_tools, 1):
            print(f"  {i}. {tool_name}: {metric.calculate_weighted_score():.1f}åˆ†")
        
        # æ€§èƒ½åˆ†æ
        print(f"\n{self.generate_performance_analysis(metrics)}")
        
        # ä½¿ç”¨å»ºè®®
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        best_tool = sorted_tools[0]
        print(f"  æ¨èä½¿ç”¨: {best_tool[0]} (ç»¼åˆè¯„åˆ†: {best_tool[1].calculate_weighted_score():.1f})")
        
        for tool_name, metric in sorted_tools:
            # è®¡ç®—æˆåŠŸç‡
            successful_count = sum(1 for result in metric.sample_results if result.conversion_success)
            success_rate = (successful_count / len(metric.sample_results) * 100) if metric.sample_results else 0
            
            if success_rate >= 90:
                reliability = "é«˜å¯é æ€§"
            elif success_rate >= 70:
                reliability = "ä¸­ç­‰å¯é æ€§"
            else:
                reliability = "ä½å¯é æ€§"
            
            # è®¡ç®—å¹³å‡è½¬æ¢æ—¶é—´
            avg_time = sum(metric.conversion_times) / len(metric.conversion_times) if metric.conversion_times else 0
            
            print(f"  â€¢ {tool_name}: {reliability}ï¼Œé€‚åˆ", end="")
            if success_rate >= 90 and avg_time <= 3:
                print("ç”Ÿäº§ç¯å¢ƒä½¿ç”¨")
            elif success_rate >= 70:
                print("ä¸€èˆ¬ç”¨é€”")
            else:
                print("æµ‹è¯•ç¯å¢ƒæˆ–ç‰¹å®šåœºæ™¯")
        
        print("="*60)
        print()
    
    def print_objective_evaluation_report(self, objective_metrics: Dict[str, ObjectiveMetrics]) -> None:
        """æ‰“å°å®¢è§‚è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š å®¢è§‚è¯„ä¼°æŠ¥å‘Š")
        print("="*60)
        
        for tool_name, metrics in objective_metrics.items():
            print(f"\nğŸ”§ {tool_name}")
            print("-" * 40)
            print(f"ç»¼åˆè¯„åˆ†: {metrics.overall_score:.1f}/100")
            print(f"å¹³å‡æ–‡ä»¶å¤§å°: {metrics.avg_file_size:.1f}KB")
            print(f"å‹ç¼©æ•ˆç‡: {metrics.compression_efficiency:.1f}%")
            print(f"æ–‡æœ¬ä¿ç•™ç‡: {metrics.text_preservation_rate:.1f}%")
            print(f"å†…å®¹å¯†åº¦: {metrics.content_density_score:.1f}%")
            print(f"ä¸­æ–‡æ”¯æŒ: {metrics.chinese_support_score:.1f}%")
            print(f"ç‰¹æ®Šå­—ç¬¦æ”¯æŒ: {metrics.special_char_support:.1f}%")
            print(f"å›¾ç‰‡æ”¯æŒç‡: {metrics.image_support_rate:.1f}%")
            print(f"å­—ä½“ä¿ç•™ç‡: {metrics.font_preservation_rate:.1f}%")
            print(f"è¡¨å•æ”¯æŒç‡: {metrics.form_support_rate:.1f}%")
            print(f"é¡µé¢ç»“æ„: {metrics.page_structure_score:.1f}%")
            print(f"æˆåŠŸç‡: {metrics.success_rate:.1f}%")
            print(f"é”™è¯¯ç‡: {metrics.error_rate:.1f}%")
        
        print("\n" + "="*60)
        print()
    
    def generate_html_report(self, results: Dict[str, List[SampleResult]], 
                           metrics: Dict[str, EvaluationMetrics],
                           objective_metrics: Dict[str, ObjectiveMetrics] = None) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        return self.html_generator.generate_full_report(results, metrics, objective_metrics)
    
    def save_results(self, results: Dict[str, List[SampleResult]], 
                    metrics: Dict[str, EvaluationMetrics], 
                    objective_metrics: Dict[str, ObjectiveMetrics] = None) -> None:
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜JSONç»“æœ
        json_data = {
            "results": {
                tool_name: [
                    {
                        "sample_name": r.sample_name,
                        "file_path": r.file_path,
                        "conversion_success": r.conversion_success,
                        "conversion_time": r.conversion_time,
                        "file_size": r.file_size,
                        "quality_score": r.quality_score,
                        "error_message": r.error_message,
                        "notes": r.notes
                    }
                    for r in tool_results
                ]
                for tool_name, tool_results in results.items()
            },
            "metrics": {
                tool_name: {
                    "tool_name": m.tool_name,
                    "layout_visual_score": m.layout_visual_score,
                    "functionality_score": m.functionality_score,
                    "performance_score": m.performance_score,
                    "deployment_score": m.deployment_score,
                    "customization_score": m.customization_score,
                    "weighted_score": m.calculate_weighted_score(),
                    "conversion_times": m.conversion_times,
                    "file_sizes": m.file_sizes
                }
                for tool_name, m in metrics.items()
            }
        }
        
        # æ·»åŠ å®¢è§‚è¯„ä¼°ç»“æœ
        if objective_metrics:
            json_data["objective_metrics"] = {
                tool_name: {
                    "tool_name": om.tool_name,
                    "overall_score": om.overall_score,
                    "avg_file_size": om.avg_file_size,
                    "file_size_consistency": om.file_size_consistency,
                    "compression_efficiency": om.compression_efficiency,
                    "text_preservation_rate": om.text_preservation_rate,
                    "content_density_score": om.content_density_score,
                    "chinese_support_score": om.chinese_support_score,
                    "special_char_support": om.special_char_support,
                    "image_support_rate": om.image_support_rate,
                    "font_preservation_rate": om.font_preservation_rate,
                    "form_support_rate": om.form_support_rate,
                    "page_structure_score": om.page_structure_score,
                    "success_rate": om.success_rate,
                    "error_rate": om.error_rate
                }
                for tool_name, om in objective_metrics.items()
            }
        
        json_path = f"{self.output_dir}/evaluation_results.json"
        self.file_ops.save_json(json_data, json_path)
        print(f"ğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {json_path}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜HTMLæŠ¥å‘Š
        html_content = self.generate_html_report(results, metrics, objective_metrics)
        html_path = f"{self.output_dir}/evaluation_report.html"
        self.file_ops.save_text(html_content, html_path)
        print(f"ğŸŒ HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_path}")
    
    def run_complete_evaluation(self) -> None:
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        # è¿è¡Œè¯„ä¼°
        results, metrics, objective_metrics = self.run_evaluation()
        
        # æ‰“å°ä¼ ç»Ÿè¯„ä¼°æŠ¥å‘Š
        self.print_evaluation_report(results, metrics)
        
        # æ‰“å°å®¢è§‚è¯„ä¼°æŠ¥å‘Š
        self.print_objective_evaluation_report(objective_metrics)
        
        # ä¿å­˜ç»“æœ
        self.save_results(results, metrics, objective_metrics)